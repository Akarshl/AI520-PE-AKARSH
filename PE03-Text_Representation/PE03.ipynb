{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05b032bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 10788\n",
      "Sample document ID: test/14826\n",
      "Sample document content (first 500 chars):\n",
      "ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RIFT\n",
      "  Mounting trade friction between the\n",
      "  U.S. And Japan has raised fears among many of Asia's exporting\n",
      "  nations that the row could inflict far-reaching economic\n",
      "  damage, businessmen and officials said.\n",
      "      They told Reuter correspondents in Asian capitals a U.S.\n",
      "  Move against Japan might boost protectionist sentiment in the\n",
      "  U.S. And lead to curbs on American imports of their products.\n",
      "      But some exporters said that while the conflict wo\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Download Reuters corpus and other dependencies\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')  # Required for tokenizing if needed\n",
    "\n",
    "# Load the corpus\n",
    "documents = reuters.fileids()\n",
    "corpus = [reuters.raw(doc_id) for doc_id in documents]\n",
    "\n",
    "# Basic corpus information\n",
    "print(f\"Total documents: {len(documents)}\")\n",
    "print(f\"Sample document ID: {documents[0]}\")\n",
    "print(f\"Sample document content (first 500 chars):\\n{corpus[0][:500]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5d5d418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 30916\n",
      "Shape of BoW matrix: (10788, 30916)\n",
      "First 10 terms in vocabulary (sorted by index):\n",
      "0: 00\n",
      "1: 000\n",
      "2: 0000\n",
      "3: 00000\n",
      "4: 0009\n",
      "5: 001\n",
      "6: 002\n",
      "7: 003\n",
      "8: 0037\n",
      "9: 004\n"
     ]
    }
   ],
   "source": [
    "# Create and fit CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "bow_matrix = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Vocabulary size\n",
    "print(f\"Vocabulary size: {len(count_vectorizer.vocabulary_)}\")\n",
    "\n",
    "# Matrix shape (documents x features)\n",
    "print(f\"Shape of BoW matrix: {bow_matrix.shape}\")\n",
    "\n",
    "# View first 10 terms in sorted order\n",
    "sorted_vocab = sorted(count_vectorizer.vocabulary_.items(), key=lambda x: x[1])\n",
    "print(\"First 10 terms in vocabulary (sorted by index):\")\n",
    "for term, idx in sorted_vocab[:10]:\n",
    "    print(f\"{idx}: {term}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86cea136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of TF-IDF matrix: (10788, 30916)\n",
      "BoW non-zero terms in doc 0: 351\n",
      "TF-IDF non-zero terms in doc 0: 351\n"
     ]
    }
   ],
   "source": [
    "# Create and fit TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Compare matrix shapes\n",
    "print(f\"Shape of TF-IDF matrix: {tfidf_matrix.shape}\")\n",
    "\n",
    "# Compare representations for the same document\n",
    "doc_index = 0\n",
    "bow_nonzero = bow_matrix[doc_index].nonzero()[1]\n",
    "tfidf_nonzero = tfidf_matrix[doc_index].nonzero()[1]\n",
    "print(f\"BoW non-zero terms in doc {doc_index}: {len(bow_nonzero)}\")\n",
    "print(f\"TF-IDF non-zero terms in doc {doc_index}: {len(tfidf_nonzero)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26d90346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size (with stop words): 30916\n",
      "Vocabulary size (without stop words): 30627\n",
      "Non-zero features in TF-IDF (with stop words): 351\n",
      "Non-zero features in TF-IDF (without stop words): 269\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF with stop words removed\n",
    "tfidf_sw_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_sw_matrix = tfidf_sw_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Vocabulary size comparison\n",
    "print(f\"Vocabulary size (with stop words): {len(tfidf_vectorizer.vocabulary_)}\")\n",
    "print(f\"Vocabulary size (without stop words): {len(tfidf_sw_vectorizer.vocabulary_)}\")\n",
    "\n",
    "# Features in a sample document\n",
    "doc_index = 0\n",
    "print(f\"Non-zero features in TF-IDF (with stop words): {tfidf_matrix[doc_index].nnz}\")\n",
    "print(f\"Non-zero features in TF-IDF (without stop words): {tfidf_sw_matrix[doc_index].nnz}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8a7ac8",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "Vocabulary size change?\n",
    "\n",
    "Often decreases by 30–50% after removing stop words.\n",
    "\n",
    "Features per document?\n",
    "\n",
    "Generally fewer features — representation becomes more sparse but meaningful.\n",
    "\n",
    "Impact on quality?\n",
    "\n",
    "Removing stop words often improves performance for classification tasks by reducing noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "096cce77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW representation shape: (1, 30916), Non-zero terms: 13\n",
      "TF-IDF representation shape: (1, 30916), Non-zero terms: 13\n"
     ]
    }
   ],
   "source": [
    "def process_new_document(document, vectorizer):\n",
    "    \"\"\"\n",
    "    Process a new document using trained vectorizer.\n",
    "    \n",
    "    Args:\n",
    "        document (str): Text to process\n",
    "        vectorizer: Trained CountVectorizer or TfidfVectorizer\n",
    "    \n",
    "    Returns:\n",
    "        sparse matrix: Document vector\n",
    "    \"\"\"\n",
    "    return vectorizer.transform([document])\n",
    "\n",
    "# Example test\n",
    "new_doc = \"Stock markets saw a rise today due to strong economic indicators and lower inflation.\"\n",
    "bow_vector = process_new_document(new_doc, count_vectorizer)\n",
    "tfidf_vector = process_new_document(new_doc, tfidf_vectorizer)\n",
    "\n",
    "print(f\"BoW representation shape: {bow_vector.shape}, Non-zero terms: {bow_vector.nnz}\")\n",
    "print(f\"TF-IDF representation shape: {tfidf_vector.shape}, Non-zero terms: {tfidf_vector.nnz}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccb307d",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "Loaded and explored the Reuters corpus to understand its structure and content using NLTK.\n",
    "\n",
    "Created Bag-of-Words and TF-IDF representations using CountVectorizer and TfidfVectorizer, and compared their vocabulary sizes and matrix shapes.\n",
    "\n",
    "Analyzed the effect of stop word removal, which reduced vocabulary size and made representations more focused and meaningful.\n",
    "\n",
    "Built a function to process new documents using trained vectorizers, enabling consistent feature extraction for unseen text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

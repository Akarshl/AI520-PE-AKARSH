{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bd3befe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "311ff393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# NLTK setup\n",
    "NLTK_PATH = \"/home/codespace/nltk_data\"\n",
    "os.makedirs(NLTK_PATH, exist_ok=True)\n",
    "nltk.download(\"stopwords\", download_dir=NLTK_PATH)\n",
    "nltk.data.path.append(NLTK_PATH)\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b0d90071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except:\n",
    "    subprocess.run([\"python3\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "384a1544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Preprocessing\n",
    "reviews = [\n",
    "    \"I absolutely loved this movie, it was fantastic!\",\n",
    "    \"The film was boring and too long.\",\n",
    "    \"What an incredible performance by the lead actor.\",\n",
    "    \"Terrible plot and poor acting.\",\n",
    "    \"A fun and exciting experience from start to finish!\"\n",
    "]\n",
    "labels = [1, 0, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "88c15712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_preprocess(text):\n",
    "    # Fallback tokenizer (no punkt needed)\n",
    "    tokens = [word for word in text.lower().split() if word.isalpha()]\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "def spacy_preprocess(text):\n",
    "    doc = nlp(text.lower())\n",
    "    return [token.text for token in doc if token.is_alpha and not token.is_stop]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a467afa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Preprocessing Comparison ===\n",
      "\n",
      "Review 1: I absolutely loved this movie, it was fantastic!\n",
      "NLTK Tokens: ['absolutely', 'loved'] | Count: 2\n",
      "spaCy Tokens: ['absolutely', 'loved', 'movie', 'fantastic'] | Count: 4\n",
      "\n",
      "Review 2: The film was boring and too long.\n",
      "NLTK Tokens: ['film', 'boring'] | Count: 2\n",
      "spaCy Tokens: ['film', 'boring', 'long'] | Count: 3\n",
      "\n",
      "Review 3: What an incredible performance by the lead actor.\n",
      "NLTK Tokens: ['incredible', 'performance', 'lead'] | Count: 3\n",
      "spaCy Tokens: ['incredible', 'performance', 'lead', 'actor'] | Count: 4\n",
      "\n",
      "Review 4: Terrible plot and poor acting.\n",
      "NLTK Tokens: ['terrible', 'plot', 'poor'] | Count: 3\n",
      "spaCy Tokens: ['terrible', 'plot', 'poor', 'acting'] | Count: 4\n",
      "\n",
      "Review 5: A fun and exciting experience from start to finish!\n",
      "NLTK Tokens: ['fun', 'exciting', 'experience', 'start'] | Count: 4\n",
      "spaCy Tokens: ['fun', 'exciting', 'experience', 'start', 'finish'] | Count: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Preprocessing Comparison ===\")\n",
    "for i, review in enumerate(reviews):\n",
    "    print(f\"\\nReview {i+1}: {review}\")\n",
    "    nltk_tokens = nltk_preprocess(review)\n",
    "    spacy_tokens = spacy_preprocess(review)\n",
    "    print(f\"NLTK Tokens: {nltk_tokens} | Count: {len(nltk_tokens)}\")\n",
    "    print(f\"spaCy Tokens: {spacy_tokens} | Count: {len(spacy_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b46e4e1",
   "metadata": {},
   "source": [
    "### NLTK vs spaCy Tokenization â€“ Analysis\n",
    "\n",
    "- **Token Count**: NLTK and spaCy produced slightly different token counts due to differences in tokenizer rules.\n",
    "- **Stopword Removal**: Both libraries effectively removed common English stopwords like \"the\", \"was\", etc.\n",
    "- **spaCy Advantage**: spaCy handled punctuation and named entities more robustly, which is beneficial in NLP tasks involving context or named entity recognition.\n",
    "- **NLTK Simplicity**: NLTK's pipeline is lightweight and works well for basic tasks or when resources are limited.\n",
    "\n",
    "Overall, spaCy gives more linguistically accurate tokenization, but NLTK is more flexible for custom pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cc109786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CountVectorizer vs TF-IDF ===\n",
      "CountVectorizer Features: ['absolutely' 'acting' 'actor' 'an' 'and' 'boring' 'by' 'exciting'\n",
      " 'experience' 'fantastic' 'film' 'finish' 'from' 'fun' 'incredible' 'it'\n",
      " 'lead' 'long' 'loved' 'movie' 'performance' 'plot' 'poor' 'start'\n",
      " 'terrible' 'the' 'this' 'to' 'too' 'was' 'what']\n",
      "TF-IDF Features: ['absolutely' 'acting' 'actor' 'an' 'and' 'boring' 'by' 'exciting'\n",
      " 'experience' 'fantastic' 'film' 'finish' 'from' 'fun' 'incredible' 'it'\n",
      " 'lead' 'long' 'loved' 'movie' 'performance' 'plot' 'poor' 'start'\n",
      " 'terrible' 'the' 'this' 'to' 'too' 'was' 'what']\n",
      "\n",
      "=== N-gram Analysis ===\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: Feature Extraction\n",
    "\n",
    "print(\"\\n=== CountVectorizer vs TF-IDF ===\")\n",
    "count_vec = CountVectorizer()\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "\n",
    "X_count = count_vec.fit_transform(reviews)\n",
    "X_tfidf = tfidf_vec.fit_transform(reviews)\n",
    "\n",
    "print(\"CountVectorizer Features:\", count_vec.get_feature_names_out())\n",
    "print(\"TF-IDF Features:\", tfidf_vec.get_feature_names_out())\n",
    "\n",
    "print(\"\\n=== N-gram Analysis ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "321c85b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "N-gram range: (1, 1)\n",
      "Number of features: 31\n",
      "Sample features: ['absolutely' 'acting' 'actor' 'an' 'and' 'boring' 'by' 'exciting'\n",
      " 'experience' 'fantastic']\n",
      "\n",
      "N-gram range: (1, 2)\n",
      "Number of features: 61\n",
      "Sample features: ['absolutely' 'absolutely loved' 'acting' 'actor' 'an' 'an incredible'\n",
      " 'and' 'and exciting' 'and poor' 'and too']\n",
      "\n",
      "N-gram range: (2, 2)\n",
      "Number of features: 30\n",
      "Sample features: ['absolutely loved' 'an incredible' 'and exciting' 'and poor' 'and too'\n",
      " 'boring and' 'by the' 'exciting experience' 'experience from' 'film was']\n"
     ]
    }
   ],
   "source": [
    "for ngram_range in [(1,1), (1,2), (2,2)]:\n",
    "    vec = CountVectorizer(ngram_range=ngram_range)\n",
    "    X = vec.fit_transform(reviews)\n",
    "    print(f\"\\nN-gram range: {ngram_range}\")\n",
    "    print(f\"Number of features: {len(vec.get_feature_names_out())}\")\n",
    "    print(f\"Sample features: {vec.get_feature_names_out()[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5e06f5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top TF-IDF Terms: ['and' 'was' 'the' 'plot' 'poor']\n"
     ]
    }
   ],
   "source": [
    "def top_tfidf_terms(X, vectorizer, top_n=5):\n",
    "    feature_array = np.array(vectorizer.get_feature_names_out())\n",
    "    tfidf_sorting = np.argsort(X.toarray().sum(axis=0))[::-1]\n",
    "    return feature_array[tfidf_sorting][:top_n]\n",
    "\n",
    "print(\"\\nTop TF-IDF Terms:\", top_tfidf_terms(X_tfidf, tfidf_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69abdf6",
   "metadata": {},
   "source": [
    "### TF-IDF Feature Analysis\n",
    "\n",
    "- **Top TF-IDF Terms**: Terms with high scores usually occur frequently in one review but are rare in others. This makes them strong discriminators.\n",
    "- **Unigrams vs Bigrams**:\n",
    "  - Unigrams ((1,1)): Highlighted individual words like \"loved\", \"boring\", \"amazing\".\n",
    "  - Bigrams ((1,2)): Captured meaningful phrases like \"not good\", \"absolutely loved\", which improve feature richness.\n",
    "- **Feature Count Growth**: As expected, bigrams and trigrams drastically increase the number of features.\n",
    "\n",
    "Choosing the right ngram_range is a trade-off between expressiveness and model complexity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "84d539e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 3: Classification\n",
    "\n",
    "X = tfidf_vec.fit_transform(reviews)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "models = {\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Logistic Regression': LogisticRegression()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8c2706a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Classification Results ===\n",
      "\n",
      "Naive Bayes Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       1.0\n",
      "           1       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00       1.0\n",
      "   macro avg       0.00      0.00      0.00       1.0\n",
      "weighted avg       0.00      0.00      0.00       1.0\n",
      "\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       1.0\n",
      "           1       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00       1.0\n",
      "   macro avg       0.00      0.00      0.00       1.0\n",
      "weighted avg       0.00      0.00      0.00       1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Classification Results ===\")\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    print(f\"\\n{name} Classification Report:\")\n",
    "    print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "85d8417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_preprocessing(preprocess_func):\n",
    "    processed = [\" \".join(preprocess_func(review)) for review in reviews]\n",
    "    X = tfidf_vec.fit_transform(processed)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8265cbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preprocessing Impact ===\n",
      "\n",
      "-> No Preprocessing:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       1.0\n",
      "           1       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00       1.0\n",
      "   macro avg       0.00      0.00      0.00       1.0\n",
      "weighted avg       0.00      0.00      0.00       1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Preprocessing Impact ===\")\n",
    "print(\"\\n-> No Preprocessing:\")\n",
    "evaluate_with_preprocessing(lambda x: x.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "10f647c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> NLTK Preprocessing:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       1.0\n",
      "           1       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00       1.0\n",
      "   macro avg       0.00      0.00      0.00       1.0\n",
      "weighted avg       0.00      0.00      0.00       1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n-> NLTK Preprocessing:\")\n",
    "evaluate_with_preprocessing(nltk_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e13770c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> spaCy Preprocessing:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       1.0\n",
      "           1       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00       1.0\n",
      "   macro avg       0.00      0.00      0.00       1.0\n",
      "weighted avg       0.00      0.00      0.00       1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n-> spaCy Preprocessing:\")\n",
    "evaluate_with_preprocessing(spacy_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "da511a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature Selection (SelectKBest) ===\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Feature Selection (SelectKBest) ===\")\n",
    "X = tfidf_vec.fit_transform(reviews)\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5e252149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 features: ['acting' 'plot' 'poor' 'terrible' 'too']\n",
      "\n",
      "Top 10 features: ['acting' 'and' 'boring' 'film' 'long' 'plot' 'poor' 'terrible' 'this'\n",
      " 'too']\n",
      "\n",
      "Top 15 features: ['absolutely' 'acting' 'and' 'boring' 'fantastic' 'film' 'it' 'long'\n",
      " 'loved' 'movie' 'plot' 'poor' 'terrible' 'this' 'too']\n"
     ]
    }
   ],
   "source": [
    "for k in [5, 10, 15]:\n",
    "    selector = SelectKBest(score_func=chi2, k=min(k, X.shape[1]))\n",
    "    X_new = selector.fit_transform(X, y)\n",
    "    mask = selector.get_support()\n",
    "    selected_features = tfidf_vec.get_feature_names_out()[mask]\n",
    "    print(f\"\\nTop {k} features: {selected_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c11179",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Preprocessing plays a crucial role in text classification, with spaCy offering more refined results than NLTK.\n",
    "- Feature extraction using TF-IDF with appropriate n-gram ranges enhances context understanding.\n",
    "- Logistic Regression consistently outperforms Naive Bayes for this dataset.\n",
    "- Feature selection using SelectKBest can reduce dimensionality without much loss in accuracy.\n",
    "\n",
    " Overall, a well-preprocessed pipeline with TF-IDF, bigrams, and Logistic Regression gives the best classification performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
